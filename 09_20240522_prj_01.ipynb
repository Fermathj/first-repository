{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbd31442",
   "metadata": {},
   "source": [
    "**(0) 문제정의**  \n",
    "손수 설계하는 선형회귀, 당뇨병 수치를 맞춰보자!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4f2489",
   "metadata": {},
   "source": [
    "- data => (442,10) 크기의 ndarray, 입력값\n",
    "- target => (442, ) 크기의 ndarray, 타깃\n",
    "- feature_names => data의 각 열의 특징 이름\n",
    "- frame => (442,11)크기의 데이터 프레임(호출할 때 인자로 받은 as_frame=True일 때만 접근 가능)\n",
    "- DESCR => 데이터셋의 설명\n",
    "- data_filename => data의 위치경로\n",
    "- target_filename => target의 위치경로\n",
    "- (data,target) => 입력값과 타깃 튜플 (return_X_y=True일 때만 접근 가능)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8329756b",
   "metadata": {},
   "source": [
    "**(1) 데이터 가져오기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dd1bac1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sklearn\n",
    "from sklearn.datasets import load_diabetes\n",
    "diabetes=load_diabetes()\n",
    "\n",
    "#diabetes\n",
    "\n",
    "df_X=diabetes.data\n",
    "df_y=diabetes.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79826dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature 1 : age\n",
      "feature 2 : sex\n",
      "feature 3 : bmi\n",
      "feature 4 : bp\n",
      "feature 5 : s1\n",
      "feature 6 : s2\n",
      "feature 7 : s3\n",
      "feature 8 : s4\n",
      "feature 9 : s5\n",
      "feature 10 : s6\n"
     ]
    }
   ],
   "source": [
    "# feature 확인 \n",
    "for i,feature_name in enumerate(diabetes.feature_names):\n",
    "  print(f'feature {i+1} : {feature_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e54b037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442, 10)\n",
      "(442,)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 확인 --> 442 개의 row, 10개의 feature \n",
    "print(df_X.shape)\n",
    "print(df_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1aa5b856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<x_data[0]> :  [ 0.03807591  0.05068012  0.06169621  0.02187235 -0.0442235  -0.03482076\n",
      " -0.04340085 -0.00259226  0.01990842 -0.01764613]\n",
      "\n",
      "<y_data[0]> :  151.0\n"
     ]
    }
   ],
   "source": [
    "# 데아터 탐색을 통해 더미변수를 사용할지 체크, 모든 특성이 -0.2 ~ 0.2 사이에 분포하도록 조정됨\n",
    "print('<df_X[0]> : ',df_X[0])\n",
    "print()\n",
    "print('<df_y[0]> : ',df_y[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c9cd36",
   "metadata": {},
   "source": [
    "**(2) 모델에 입력할 데이터 X 준비하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b892cbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "X = np.array(df_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9147bf82",
   "metadata": {},
   "source": [
    "**(3) 모델에 예측할 데이터 y 준비하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24688efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(df_y)  : df_y\n",
    "y = np.array(df_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a35e4f",
   "metadata": {},
   "source": [
    "**(4) train 데이터와 test 데이터로 분리하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86e25f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(353, 10) (353,)\n",
      "(89, 10) (89,)\n"
     ]
    }
   ],
   "source": [
    "#  sklearn .train_test_split 을 사용한 train 과 test 데이터 분리\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfc1de1",
   "metadata": {},
   "source": [
    "**(5) 모델 준비하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92e93a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 데이터 개수에 맞는 가중치 W와 b를 준비해주세요.\n",
    "\n",
    "W = np.random.rand(10)\n",
    "b = np.random.rand()\n",
    "\n",
    "# 모델준비\n",
    "def model(X, W, b):\n",
    "    predictions = 0\n",
    "    for i in range(10):\n",
    "        predictions += X[:, i] * W[i]\n",
    "    predictions += b\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fef82a3",
   "metadata": {},
   "source": [
    "**(6) 손실함수 loss 정의하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f5e59ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE 사용하여  손실함수 계산\n",
    "def MSE(a, b):\n",
    "    mse = ((a - b) ** 2).mean()  # 두 값의 차이의 제곱의 평균\n",
    "    return mse\n",
    "\n",
    "## error 계산 \n",
    "def loss(X, W, b, y):\n",
    "    predictions = model(X, W, b)\n",
    "    L = MSE(predictions, y)\n",
    "    return L\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4bea47",
   "metadata": {},
   "source": [
    "**(7) 기울기를 구하는 gradient 함수 구현하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8248b037",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 경사하강법에 의한 gradient를 다변수 방정식의 편미분을 이해하고 구현하기\n",
    "def gradient(X, W, b, y):\n",
    "    # N은 데이터 포인트의 개수\n",
    "    N = len(y)\n",
    "    \n",
    "    # y_pred 준비\n",
    "    y_pred = model(X, W, b)\n",
    "    \n",
    "    # 공식에 맞게 gradient 계산\n",
    "    dW = 1/N * 2 * X.T.dot(y_pred - y)\n",
    "        \n",
    "    # b의 gradient 계산\n",
    "    db = 2 * (y_pred - y).mean()\n",
    "    return dW, db\n",
    "\n",
    "#dW, db = gradient(X, W, b, y)\n",
    "#print(\"dW:\", dW)\n",
    "#print(\"db:\", db)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a865db5b",
   "metadata": {},
   "source": [
    "**(8) 하이퍼 파라미터인 학습률 설정하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "22ef1bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습률\n",
    "LEARNING_RATE = 0.01  # 0.0001 이면 loss 가 너무 크다  줄이면 loss 가 작아진다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf40d6b",
   "metadata": {},
   "source": [
    "**(9) 모델 학습하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "75837eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10 : Loss 4703.4496\n",
      "Iteration 20 : Loss 4699.6594\n",
      "Iteration 30 : Loss 4695.8801\n",
      "Iteration 40 : Loss 4692.1118\n",
      "Iteration 50 : Loss 4688.3544\n",
      "Iteration 60 : Loss 4684.6079\n",
      "Iteration 70 : Loss 4680.8722\n",
      "Iteration 80 : Loss 4677.1474\n",
      "Iteration 90 : Loss 4673.4333\n",
      "Iteration 100 : Loss 4669.7299\n",
      "Iteration 110 : Loss 4666.0372\n",
      "Iteration 120 : Loss 4662.3552\n",
      "Iteration 130 : Loss 4658.6838\n",
      "Iteration 140 : Loss 4655.0230\n",
      "Iteration 150 : Loss 4651.3728\n",
      "Iteration 160 : Loss 4647.7330\n",
      "Iteration 170 : Loss 4644.1038\n",
      "Iteration 180 : Loss 4640.4850\n",
      "Iteration 190 : Loss 4636.8766\n",
      "Iteration 200 : Loss 4633.2786\n",
      "Iteration 210 : Loss 4629.6909\n",
      "Iteration 220 : Loss 4626.1136\n",
      "Iteration 230 : Loss 4622.5465\n",
      "Iteration 240 : Loss 4618.9896\n",
      "Iteration 250 : Loss 4615.4430\n",
      "Iteration 260 : Loss 4611.9065\n",
      "Iteration 270 : Loss 4608.3802\n",
      "Iteration 280 : Loss 4604.8640\n",
      "Iteration 290 : Loss 4601.3579\n",
      "Iteration 300 : Loss 4597.8617\n",
      "Iteration 310 : Loss 4594.3756\n",
      "Iteration 320 : Loss 4590.8995\n",
      "Iteration 330 : Loss 4587.4333\n",
      "Iteration 340 : Loss 4583.9770\n",
      "Iteration 350 : Loss 4580.5306\n",
      "Iteration 360 : Loss 4577.0940\n",
      "Iteration 370 : Loss 4573.6672\n",
      "Iteration 380 : Loss 4570.2502\n",
      "Iteration 390 : Loss 4566.8430\n",
      "Iteration 400 : Loss 4563.4454\n",
      "Iteration 410 : Loss 4560.0575\n",
      "Iteration 420 : Loss 4556.6793\n",
      "Iteration 430 : Loss 4553.3106\n",
      "Iteration 440 : Loss 4549.9516\n",
      "Iteration 450 : Loss 4546.6021\n",
      "Iteration 460 : Loss 4543.2621\n",
      "Iteration 470 : Loss 4539.9316\n",
      "Iteration 480 : Loss 4536.6106\n",
      "Iteration 490 : Loss 4533.2989\n",
      "Iteration 500 : Loss 4529.9967\n",
      "Iteration 510 : Loss 4526.7038\n",
      "Iteration 520 : Loss 4523.4203\n",
      "Iteration 530 : Loss 4520.1460\n",
      "Iteration 540 : Loss 4516.8810\n",
      "Iteration 550 : Loss 4513.6253\n",
      "Iteration 560 : Loss 4510.3787\n",
      "Iteration 570 : Loss 4507.1413\n",
      "Iteration 580 : Loss 4503.9131\n",
      "Iteration 590 : Loss 4500.6940\n",
      "Iteration 600 : Loss 4497.4840\n",
      "Iteration 610 : Loss 4494.2830\n",
      "Iteration 620 : Loss 4491.0910\n",
      "Iteration 630 : Loss 4487.9080\n",
      "Iteration 640 : Loss 4484.7340\n",
      "Iteration 650 : Loss 4481.5690\n",
      "Iteration 660 : Loss 4478.4128\n",
      "Iteration 670 : Loss 4475.2655\n",
      "Iteration 680 : Loss 4472.1271\n",
      "Iteration 690 : Loss 4468.9974\n",
      "Iteration 700 : Loss 4465.8766\n",
      "Iteration 710 : Loss 4462.7645\n",
      "Iteration 720 : Loss 4459.6611\n",
      "Iteration 730 : Loss 4456.5665\n",
      "Iteration 740 : Loss 4453.4805\n",
      "Iteration 750 : Loss 4450.4032\n",
      "Iteration 760 : Loss 4447.3344\n",
      "Iteration 770 : Loss 4444.2743\n",
      "Iteration 780 : Loss 4441.2227\n",
      "Iteration 790 : Loss 4438.1796\n",
      "Iteration 800 : Loss 4435.1451\n",
      "Iteration 810 : Loss 4432.1190\n",
      "Iteration 820 : Loss 4429.1013\n",
      "Iteration 830 : Loss 4426.0921\n",
      "Iteration 840 : Loss 4423.0913\n",
      "Iteration 850 : Loss 4420.0988\n",
      "Iteration 860 : Loss 4417.1146\n",
      "Iteration 870 : Loss 4414.1388\n",
      "Iteration 880 : Loss 4411.1712\n",
      "Iteration 890 : Loss 4408.2119\n",
      "Iteration 900 : Loss 4405.2609\n",
      "Iteration 910 : Loss 4402.3180\n",
      "Iteration 920 : Loss 4399.3832\n",
      "Iteration 930 : Loss 4396.4567\n",
      "Iteration 940 : Loss 4393.5382\n",
      "Iteration 950 : Loss 4390.6278\n",
      "Iteration 960 : Loss 4387.7255\n",
      "Iteration 970 : Loss 4384.8313\n",
      "Iteration 980 : Loss 4381.9450\n",
      "Iteration 990 : Loss 4379.0667\n",
      "Iteration 1000 : Loss 4376.1964\n",
      "Iteration 1010 : Loss 4373.3340\n",
      "Iteration 1020 : Loss 4370.4796\n",
      "Iteration 1030 : Loss 4367.6329\n",
      "Iteration 1040 : Loss 4364.7942\n",
      "Iteration 1050 : Loss 4361.9633\n",
      "Iteration 1060 : Loss 4359.1402\n",
      "Iteration 1070 : Loss 4356.3248\n",
      "Iteration 1080 : Loss 4353.5172\n",
      "Iteration 1090 : Loss 4350.7173\n",
      "Iteration 1100 : Loss 4347.9252\n",
      "Iteration 1110 : Loss 4345.1407\n",
      "Iteration 1120 : Loss 4342.3638\n",
      "Iteration 1130 : Loss 4339.5946\n",
      "Iteration 1140 : Loss 4336.8330\n",
      "Iteration 1150 : Loss 4334.0790\n",
      "Iteration 1160 : Loss 4331.3325\n",
      "Iteration 1170 : Loss 4328.5935\n",
      "Iteration 1180 : Loss 4325.8620\n",
      "Iteration 1190 : Loss 4323.1380\n",
      "Iteration 1200 : Loss 4320.4215\n",
      "Iteration 1210 : Loss 4317.7123\n",
      "Iteration 1220 : Loss 4315.0106\n",
      "Iteration 1230 : Loss 4312.3163\n",
      "Iteration 1240 : Loss 4309.6293\n",
      "Iteration 1250 : Loss 4306.9496\n",
      "Iteration 1260 : Loss 4304.2773\n",
      "Iteration 1270 : Loss 4301.6122\n",
      "Iteration 1280 : Loss 4298.9544\n",
      "Iteration 1290 : Loss 4296.3038\n",
      "Iteration 1300 : Loss 4293.6605\n",
      "Iteration 1310 : Loss 4291.0243\n",
      "Iteration 1320 : Loss 4288.3953\n",
      "Iteration 1330 : Loss 4285.7734\n",
      "Iteration 1340 : Loss 4283.1587\n",
      "Iteration 1350 : Loss 4280.5510\n",
      "Iteration 1360 : Loss 4277.9504\n",
      "Iteration 1370 : Loss 4275.3569\n",
      "Iteration 1380 : Loss 4272.7703\n",
      "Iteration 1390 : Loss 4270.1908\n",
      "Iteration 1400 : Loss 4267.6183\n",
      "Iteration 1410 : Loss 4265.0527\n",
      "Iteration 1420 : Loss 4262.4941\n",
      "Iteration 1430 : Loss 4259.9423\n",
      "Iteration 1440 : Loss 4257.3975\n",
      "Iteration 1450 : Loss 4254.8595\n",
      "Iteration 1460 : Loss 4252.3284\n",
      "Iteration 1470 : Loss 4249.8040\n",
      "Iteration 1480 : Loss 4247.2865\n",
      "Iteration 1490 : Loss 4244.7758\n",
      "Iteration 1500 : Loss 4242.2718\n",
      "Iteration 1510 : Loss 4239.7745\n",
      "Iteration 1520 : Loss 4237.2840\n",
      "Iteration 1530 : Loss 4234.8001\n",
      "Iteration 1540 : Loss 4232.3229\n",
      "Iteration 1550 : Loss 4229.8524\n",
      "Iteration 1560 : Loss 4227.3885\n",
      "Iteration 1570 : Loss 4224.9312\n",
      "Iteration 1580 : Loss 4222.4804\n",
      "Iteration 1590 : Loss 4220.0363\n",
      "Iteration 1600 : Loss 4217.5986\n",
      "Iteration 1610 : Loss 4215.1675\n",
      "Iteration 1620 : Loss 4212.7429\n",
      "Iteration 1630 : Loss 4210.3248\n",
      "Iteration 1640 : Loss 4207.9131\n",
      "Iteration 1650 : Loss 4205.5078\n",
      "Iteration 1660 : Loss 4203.1090\n",
      "Iteration 1670 : Loss 4200.7165\n",
      "Iteration 1680 : Loss 4198.3304\n",
      "Iteration 1690 : Loss 4195.9507\n",
      "Iteration 1700 : Loss 4193.5773\n",
      "Iteration 1710 : Loss 4191.2102\n",
      "Iteration 1720 : Loss 4188.8494\n",
      "Iteration 1730 : Loss 4186.4949\n",
      "Iteration 1740 : Loss 4184.1466\n",
      "Iteration 1750 : Loss 4181.8045\n",
      "Iteration 1760 : Loss 4179.4687\n",
      "Iteration 1770 : Loss 4177.1390\n",
      "Iteration 1780 : Loss 4174.8155\n",
      "Iteration 1790 : Loss 4172.4982\n",
      "Iteration 1800 : Loss 4170.1870\n",
      "Iteration 1810 : Loss 4167.8819\n",
      "Iteration 1820 : Loss 4165.5828\n",
      "Iteration 1830 : Loss 4163.2899\n",
      "Iteration 1840 : Loss 4161.0030\n",
      "Iteration 1850 : Loss 4158.7221\n",
      "Iteration 1860 : Loss 4156.4472\n",
      "Iteration 1870 : Loss 4154.1784\n",
      "Iteration 1880 : Loss 4151.9155\n",
      "Iteration 1890 : Loss 4149.6585\n",
      "Iteration 1900 : Loss 4147.4075\n",
      "Iteration 1910 : Loss 4145.1624\n",
      "Iteration 1920 : Loss 4142.9232\n",
      "Iteration 1930 : Loss 4140.6899\n",
      "Iteration 1940 : Loss 4138.4624\n",
      "Iteration 1950 : Loss 4136.2408\n",
      "Iteration 1960 : Loss 4134.0250\n",
      "Iteration 1970 : Loss 4131.8149\n",
      "Iteration 1980 : Loss 4129.6107\n",
      "Iteration 1990 : Loss 4127.4123\n",
      "Iteration 2000 : Loss 4125.2195\n",
      "Iteration 2010 : Loss 4123.0325\n",
      "Iteration 2020 : Loss 4120.8513\n",
      "Iteration 2030 : Loss 4118.6757\n",
      "Iteration 2040 : Loss 4116.5057\n",
      "Iteration 2050 : Loss 4114.3415\n",
      "Iteration 2060 : Loss 4112.1828\n",
      "Iteration 2070 : Loss 4110.0298\n",
      "Iteration 2080 : Loss 4107.8824\n",
      "Iteration 2090 : Loss 4105.7405\n",
      "Iteration 2100 : Loss 4103.6043\n",
      "Iteration 2110 : Loss 4101.4735\n",
      "Iteration 2120 : Loss 4099.3483\n",
      "Iteration 2130 : Loss 4097.2287\n",
      "Iteration 2140 : Loss 4095.1145\n",
      "Iteration 2150 : Loss 4093.0057\n",
      "Iteration 2160 : Loss 4090.9025\n",
      "Iteration 2170 : Loss 4088.8046\n",
      "Iteration 2180 : Loss 4086.7122\n",
      "Iteration 2190 : Loss 4084.6252\n",
      "Iteration 2200 : Loss 4082.5436\n",
      "Iteration 2210 : Loss 4080.4674\n",
      "Iteration 2220 : Loss 4078.3965\n",
      "Iteration 2230 : Loss 4076.3310\n",
      "Iteration 2240 : Loss 4074.2707\n",
      "Iteration 2250 : Loss 4072.2158\n",
      "Iteration 2260 : Loss 4070.1661\n",
      "Iteration 2270 : Loss 4068.1218\n",
      "Iteration 2280 : Loss 4066.0827\n",
      "Iteration 2290 : Loss 4064.0488\n",
      "Iteration 2300 : Loss 4062.0201\n",
      "Iteration 2310 : Loss 4059.9966\n",
      "Iteration 2320 : Loss 4057.9784\n",
      "Iteration 2330 : Loss 4055.9652\n",
      "Iteration 2340 : Loss 4053.9573\n",
      "Iteration 2350 : Loss 4051.9545\n",
      "Iteration 2360 : Loss 4049.9567\n",
      "Iteration 2370 : Loss 4047.9641\n",
      "Iteration 2380 : Loss 4045.9766\n",
      "Iteration 2390 : Loss 4043.9942\n",
      "Iteration 2400 : Loss 4042.0168\n",
      "Iteration 2410 : Loss 4040.0444\n",
      "Iteration 2420 : Loss 4038.0771\n",
      "Iteration 2430 : Loss 4036.1148\n",
      "Iteration 2440 : Loss 4034.1575\n",
      "Iteration 2450 : Loss 4032.2051\n",
      "Iteration 2460 : Loss 4030.2577\n",
      "Iteration 2470 : Loss 4028.3153\n",
      "Iteration 2480 : Loss 4026.3777\n",
      "Iteration 2490 : Loss 4024.4451\n",
      "Iteration 2500 : Loss 4022.5174\n",
      "Iteration 2510 : Loss 4020.5946\n",
      "Iteration 2520 : Loss 4018.6766\n",
      "Iteration 2530 : Loss 4016.7635\n",
      "Iteration 2540 : Loss 4014.8552\n",
      "Iteration 2550 : Loss 4012.9517\n",
      "Iteration 2560 : Loss 4011.0530\n",
      "Iteration 2570 : Loss 4009.1592\n",
      "Iteration 2580 : Loss 4007.2701\n",
      "Iteration 2590 : Loss 4005.3857\n",
      "Iteration 2600 : Loss 4003.5061\n",
      "Iteration 2610 : Loss 4001.6313\n",
      "Iteration 2620 : Loss 3999.7611\n",
      "Iteration 2630 : Loss 3997.8956\n",
      "Iteration 2640 : Loss 3996.0349\n",
      "Iteration 2650 : Loss 3994.1788\n",
      "Iteration 2660 : Loss 3992.3273\n",
      "Iteration 2670 : Loss 3990.4805\n",
      "Iteration 2680 : Loss 3988.6383\n",
      "Iteration 2690 : Loss 3986.8008\n",
      "Iteration 2700 : Loss 3984.9678\n",
      "Iteration 2710 : Loss 3983.1394\n",
      "Iteration 2720 : Loss 3981.3156\n",
      "Iteration 2730 : Loss 3979.4963\n",
      "Iteration 2740 : Loss 3977.6815\n",
      "Iteration 2750 : Loss 3975.8713\n",
      "Iteration 2760 : Loss 3974.0656\n",
      "Iteration 2770 : Loss 3972.2644\n",
      "Iteration 2780 : Loss 3970.4677\n",
      "Iteration 2790 : Loss 3968.6754\n",
      "Iteration 2800 : Loss 3966.8876\n",
      "Iteration 2810 : Loss 3965.1043\n",
      "Iteration 2820 : Loss 3963.3253\n",
      "Iteration 2830 : Loss 3961.5508\n",
      "Iteration 2840 : Loss 3959.7807\n",
      "Iteration 2850 : Loss 3958.0149\n",
      "Iteration 2860 : Loss 3956.2535\n",
      "Iteration 2870 : Loss 3954.4965\n",
      "Iteration 2880 : Loss 3952.7438\n",
      "Iteration 2890 : Loss 3950.9955\n",
      "Iteration 2900 : Loss 3949.2515\n",
      "Iteration 2910 : Loss 3947.5117\n",
      "Iteration 2920 : Loss 3945.7763\n",
      "Iteration 2930 : Loss 3944.0451\n",
      "Iteration 2940 : Loss 3942.3182\n",
      "Iteration 2950 : Loss 3940.5956\n",
      "Iteration 2960 : Loss 3938.8771\n",
      "Iteration 2970 : Loss 3937.1629\n",
      "Iteration 2980 : Loss 3935.4530\n",
      "Iteration 2990 : Loss 3933.7472\n",
      "Iteration 3000 : Loss 3932.0456\n",
      "Iteration 3010 : Loss 3930.3481\n",
      "Iteration 3020 : Loss 3928.6548\n",
      "Iteration 3030 : Loss 3926.9657\n",
      "Iteration 3040 : Loss 3925.2807\n",
      "Iteration 3050 : Loss 3923.5998\n",
      "Iteration 3060 : Loss 3921.9230\n",
      "Iteration 3070 : Loss 3920.2503\n",
      "Iteration 3080 : Loss 3918.5817\n",
      "Iteration 3090 : Loss 3916.9171\n",
      "Iteration 3100 : Loss 3915.2566\n",
      "Iteration 3110 : Loss 3913.6002\n",
      "Iteration 3120 : Loss 3911.9478\n",
      "Iteration 3130 : Loss 3910.2993\n",
      "Iteration 3140 : Loss 3908.6549\n",
      "Iteration 3150 : Loss 3907.0145\n",
      "Iteration 3160 : Loss 3905.3781\n",
      "Iteration 3170 : Loss 3903.7456\n",
      "Iteration 3180 : Loss 3902.1171\n",
      "Iteration 3190 : Loss 3900.4925\n",
      "Iteration 3200 : Loss 3898.8719\n",
      "Iteration 3210 : Loss 3897.2552\n",
      "Iteration 3220 : Loss 3895.6424\n",
      "Iteration 3230 : Loss 3894.0334\n",
      "Iteration 3240 : Loss 3892.4284\n",
      "Iteration 3250 : Loss 3890.8272\n",
      "Iteration 3260 : Loss 3889.2299\n",
      "Iteration 3270 : Loss 3887.6364\n",
      "Iteration 3280 : Loss 3886.0468\n",
      "Iteration 3290 : Loss 3884.4610\n",
      "Iteration 3300 : Loss 3882.8789\n",
      "Iteration 3310 : Loss 3881.3007\n",
      "Iteration 3320 : Loss 3879.7263\n",
      "Iteration 3330 : Loss 3878.1557\n",
      "Iteration 3340 : Loss 3876.5888\n",
      "Iteration 3350 : Loss 3875.0256\n",
      "Iteration 3360 : Loss 3873.4662\n",
      "Iteration 3370 : Loss 3871.9106\n",
      "Iteration 3380 : Loss 3870.3586\n",
      "Iteration 3390 : Loss 3868.8104\n",
      "Iteration 3400 : Loss 3867.2658\n",
      "Iteration 3410 : Loss 3865.7250\n",
      "Iteration 3420 : Loss 3864.1878\n",
      "Iteration 3430 : Loss 3862.6542\n",
      "Iteration 3440 : Loss 3861.1243\n",
      "Iteration 3450 : Loss 3859.5981\n",
      "Iteration 3460 : Loss 3858.0755\n",
      "Iteration 3470 : Loss 3856.5564\n",
      "Iteration 3480 : Loss 3855.0410\n",
      "Iteration 3490 : Loss 3853.5292\n",
      "Iteration 3500 : Loss 3852.0210\n",
      "Iteration 3510 : Loss 3850.5163\n",
      "Iteration 3520 : Loss 3849.0152\n",
      "Iteration 3530 : Loss 3847.5177\n",
      "Iteration 3540 : Loss 3846.0236\n",
      "Iteration 3550 : Loss 3844.5332\n",
      "Iteration 3560 : Loss 3843.0462\n",
      "Iteration 3570 : Loss 3841.5627\n",
      "Iteration 3580 : Loss 3840.0827\n",
      "Iteration 3590 : Loss 3838.6062\n",
      "Iteration 3600 : Loss 3837.1332\n",
      "Iteration 3610 : Loss 3835.6637\n",
      "Iteration 3620 : Loss 3834.1976\n",
      "Iteration 3630 : Loss 3832.7349\n",
      "Iteration 3640 : Loss 3831.2757\n",
      "Iteration 3650 : Loss 3829.8199\n",
      "Iteration 3660 : Loss 3828.3675\n",
      "Iteration 3670 : Loss 3826.9185\n",
      "Iteration 3680 : Loss 3825.4729\n",
      "Iteration 3690 : Loss 3824.0306\n",
      "Iteration 3700 : Loss 3822.5918\n",
      "Iteration 3710 : Loss 3821.1563\n",
      "Iteration 3720 : Loss 3819.7241\n",
      "Iteration 3730 : Loss 3818.2953\n",
      "Iteration 3740 : Loss 3816.8698\n",
      "Iteration 3750 : Loss 3815.4476\n",
      "Iteration 3760 : Loss 3814.0288\n",
      "Iteration 3770 : Loss 3812.6132\n",
      "Iteration 3780 : Loss 3811.2009\n",
      "Iteration 3790 : Loss 3809.7919\n",
      "Iteration 3800 : Loss 3808.3862\n",
      "Iteration 3810 : Loss 3806.9837\n",
      "Iteration 3820 : Loss 3805.5845\n",
      "Iteration 3830 : Loss 3804.1885\n",
      "Iteration 3840 : Loss 3802.7957\n",
      "Iteration 3850 : Loss 3801.4062\n",
      "Iteration 3860 : Loss 3800.0198\n",
      "Iteration 3870 : Loss 3798.6367\n",
      "Iteration 3880 : Loss 3797.2568\n",
      "Iteration 3890 : Loss 3795.8800\n",
      "Iteration 3900 : Loss 3794.5064\n",
      "Iteration 3910 : Loss 3793.1359\n",
      "Iteration 3920 : Loss 3791.7686\n",
      "Iteration 3930 : Loss 3790.4045\n",
      "Iteration 3940 : Loss 3789.0435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3950 : Loss 3787.6856\n",
      "Iteration 3960 : Loss 3786.3308\n",
      "Iteration 3970 : Loss 3784.9791\n",
      "Iteration 3980 : Loss 3783.6305\n",
      "Iteration 3990 : Loss 3782.2850\n",
      "Iteration 4000 : Loss 3780.9426\n",
      "Iteration 4010 : Loss 3779.6033\n",
      "Iteration 4020 : Loss 3778.2669\n",
      "Iteration 4030 : Loss 3776.9337\n",
      "Iteration 4040 : Loss 3775.6035\n",
      "Iteration 4050 : Loss 3774.2763\n",
      "Iteration 4060 : Loss 3772.9521\n",
      "Iteration 4070 : Loss 3771.6310\n",
      "Iteration 4080 : Loss 3770.3128\n",
      "Iteration 4090 : Loss 3768.9976\n",
      "Iteration 4100 : Loss 3767.6855\n",
      "Iteration 4110 : Loss 3766.3763\n",
      "Iteration 4120 : Loss 3765.0700\n",
      "Iteration 4130 : Loss 3763.7667\n",
      "Iteration 4140 : Loss 3762.4664\n",
      "Iteration 4150 : Loss 3761.1690\n",
      "Iteration 4160 : Loss 3759.8745\n",
      "Iteration 4170 : Loss 3758.5830\n",
      "Iteration 4180 : Loss 3757.2944\n",
      "Iteration 4190 : Loss 3756.0087\n",
      "Iteration 4200 : Loss 3754.7258\n",
      "Iteration 4210 : Loss 3753.4459\n",
      "Iteration 4220 : Loss 3752.1688\n",
      "Iteration 4230 : Loss 3750.8946\n",
      "Iteration 4240 : Loss 3749.6233\n",
      "Iteration 4250 : Loss 3748.3548\n",
      "Iteration 4260 : Loss 3747.0892\n",
      "Iteration 4270 : Loss 3745.8264\n",
      "Iteration 4280 : Loss 3744.5664\n",
      "Iteration 4290 : Loss 3743.3093\n",
      "Iteration 4300 : Loss 3742.0549\n",
      "Iteration 4310 : Loss 3740.8034\n",
      "Iteration 4320 : Loss 3739.5546\n",
      "Iteration 4330 : Loss 3738.3086\n",
      "Iteration 4340 : Loss 3737.0655\n",
      "Iteration 4350 : Loss 3735.8250\n",
      "Iteration 4360 : Loss 3734.5874\n",
      "Iteration 4370 : Loss 3733.3525\n",
      "Iteration 4380 : Loss 3732.1203\n",
      "Iteration 4390 : Loss 3730.8909\n",
      "Iteration 4400 : Loss 3729.6642\n",
      "Iteration 4410 : Loss 3728.4402\n",
      "Iteration 4420 : Loss 3727.2189\n",
      "Iteration 4430 : Loss 3726.0004\n",
      "Iteration 4440 : Loss 3724.7845\n",
      "Iteration 4450 : Loss 3723.5713\n",
      "Iteration 4460 : Loss 3722.3608\n",
      "Iteration 4470 : Loss 3721.1530\n",
      "Iteration 4480 : Loss 3719.9478\n",
      "Iteration 4490 : Loss 3718.7453\n",
      "Iteration 4500 : Loss 3717.5455\n",
      "Iteration 4510 : Loss 3716.3482\n",
      "Iteration 4520 : Loss 3715.1536\n",
      "Iteration 4530 : Loss 3713.9617\n",
      "Iteration 4540 : Loss 3712.7723\n",
      "Iteration 4550 : Loss 3711.5856\n",
      "Iteration 4560 : Loss 3710.4015\n",
      "Iteration 4570 : Loss 3709.2199\n",
      "Iteration 4580 : Loss 3708.0409\n",
      "Iteration 4590 : Loss 3706.8646\n",
      "Iteration 4600 : Loss 3705.6908\n",
      "Iteration 4610 : Loss 3704.5195\n",
      "Iteration 4620 : Loss 3703.3508\n",
      "Iteration 4630 : Loss 3702.1847\n",
      "Iteration 4640 : Loss 3701.0211\n",
      "Iteration 4650 : Loss 3699.8600\n",
      "Iteration 4660 : Loss 3698.7015\n",
      "Iteration 4670 : Loss 3697.5454\n",
      "Iteration 4680 : Loss 3696.3919\n",
      "Iteration 4690 : Loss 3695.2409\n",
      "Iteration 4700 : Loss 3694.0924\n",
      "Iteration 4710 : Loss 3692.9464\n",
      "Iteration 4720 : Loss 3691.8028\n",
      "Iteration 4730 : Loss 3690.6618\n",
      "Iteration 4740 : Loss 3689.5232\n",
      "Iteration 4750 : Loss 3688.3870\n",
      "Iteration 4760 : Loss 3687.2533\n",
      "Iteration 4770 : Loss 3686.1221\n",
      "Iteration 4780 : Loss 3684.9933\n",
      "Iteration 4790 : Loss 3683.8669\n",
      "Iteration 4800 : Loss 3682.7430\n",
      "Iteration 4810 : Loss 3681.6214\n",
      "Iteration 4820 : Loss 3680.5023\n",
      "Iteration 4830 : Loss 3679.3856\n",
      "Iteration 4840 : Loss 3678.2713\n",
      "Iteration 4850 : Loss 3677.1593\n",
      "Iteration 4860 : Loss 3676.0498\n",
      "Iteration 4870 : Loss 3674.9426\n",
      "Iteration 4880 : Loss 3673.8378\n",
      "Iteration 4890 : Loss 3672.7353\n",
      "Iteration 4900 : Loss 3671.6352\n",
      "Iteration 4910 : Loss 3670.5375\n",
      "Iteration 4920 : Loss 3669.4421\n",
      "Iteration 4930 : Loss 3668.3490\n",
      "Iteration 4940 : Loss 3667.2583\n",
      "Iteration 4950 : Loss 3666.1699\n",
      "Iteration 4960 : Loss 3665.0837\n",
      "Iteration 4970 : Loss 3663.9999\n",
      "Iteration 4980 : Loss 3662.9184\n",
      "Iteration 4990 : Loss 3661.8392\n",
      "Iteration 5000 : Loss 3660.7623\n",
      "Iteration 5010 : Loss 3659.6877\n",
      "Iteration 5020 : Loss 3658.6153\n",
      "Iteration 5030 : Loss 3657.5452\n",
      "Iteration 5040 : Loss 3656.4774\n",
      "Iteration 5050 : Loss 3655.4118\n",
      "Iteration 5060 : Loss 3654.3484\n",
      "Iteration 5070 : Loss 3653.2873\n",
      "Iteration 5080 : Loss 3652.2285\n",
      "Iteration 5090 : Loss 3651.1718\n",
      "Iteration 5100 : Loss 3650.1174\n",
      "Iteration 5110 : Loss 3649.0652\n",
      "Iteration 5120 : Loss 3648.0152\n",
      "Iteration 5130 : Loss 3646.9675\n",
      "Iteration 5140 : Loss 3645.9219\n",
      "Iteration 5150 : Loss 3644.8785\n",
      "Iteration 5160 : Loss 3643.8373\n",
      "Iteration 5170 : Loss 3642.7982\n",
      "Iteration 5180 : Loss 3641.7614\n",
      "Iteration 5190 : Loss 3640.7267\n",
      "Iteration 5200 : Loss 3639.6941\n",
      "Iteration 5210 : Loss 3638.6637\n",
      "Iteration 5220 : Loss 3637.6355\n",
      "Iteration 5230 : Loss 3636.6094\n",
      "Iteration 5240 : Loss 3635.5854\n",
      "Iteration 5250 : Loss 3634.5636\n",
      "Iteration 5260 : Loss 3633.5439\n",
      "Iteration 5270 : Loss 3632.5262\n",
      "Iteration 5280 : Loss 3631.5108\n",
      "Iteration 5290 : Loss 3630.4974\n",
      "Iteration 5300 : Loss 3629.4861\n",
      "Iteration 5310 : Loss 3628.4769\n",
      "Iteration 5320 : Loss 3627.4698\n",
      "Iteration 5330 : Loss 3626.4647\n",
      "Iteration 5340 : Loss 3625.4618\n",
      "Iteration 5350 : Loss 3624.4609\n",
      "Iteration 5360 : Loss 3623.4620\n",
      "Iteration 5370 : Loss 3622.4653\n",
      "Iteration 5380 : Loss 3621.4705\n",
      "Iteration 5390 : Loss 3620.4779\n",
      "Iteration 5400 : Loss 3619.4872\n",
      "Iteration 5410 : Loss 3618.4986\n",
      "Iteration 5420 : Loss 3617.5120\n",
      "Iteration 5430 : Loss 3616.5275\n",
      "Iteration 5440 : Loss 3615.5449\n",
      "Iteration 5450 : Loss 3614.5644\n",
      "Iteration 5460 : Loss 3613.5859\n",
      "Iteration 5470 : Loss 3612.6093\n",
      "Iteration 5480 : Loss 3611.6348\n",
      "Iteration 5490 : Loss 3610.6623\n",
      "Iteration 5500 : Loss 3609.6917\n",
      "Iteration 5510 : Loss 3608.7231\n",
      "Iteration 5520 : Loss 3607.7565\n",
      "Iteration 5530 : Loss 3606.7918\n",
      "Iteration 5540 : Loss 3605.8291\n",
      "Iteration 5550 : Loss 3604.8684\n",
      "Iteration 5560 : Loss 3603.9096\n",
      "Iteration 5570 : Loss 3602.9527\n",
      "Iteration 5580 : Loss 3601.9978\n",
      "Iteration 5590 : Loss 3601.0448\n",
      "Iteration 5600 : Loss 3600.0938\n",
      "Iteration 5610 : Loss 3599.1447\n",
      "Iteration 5620 : Loss 3598.1974\n",
      "Iteration 5630 : Loss 3597.2521\n",
      "Iteration 5640 : Loss 3596.3087\n",
      "Iteration 5650 : Loss 3595.3672\n",
      "Iteration 5660 : Loss 3594.4276\n",
      "Iteration 5670 : Loss 3593.4899\n",
      "Iteration 5680 : Loss 3592.5541\n",
      "Iteration 5690 : Loss 3591.6201\n",
      "Iteration 5700 : Loss 3590.6880\n",
      "Iteration 5710 : Loss 3589.7578\n",
      "Iteration 5720 : Loss 3588.8294\n",
      "Iteration 5730 : Loss 3587.9030\n",
      "Iteration 5740 : Loss 3586.9783\n",
      "Iteration 5750 : Loss 3586.0555\n",
      "Iteration 5760 : Loss 3585.1346\n",
      "Iteration 5770 : Loss 3584.2154\n",
      "Iteration 5780 : Loss 3583.2981\n",
      "Iteration 5790 : Loss 3582.3827\n",
      "Iteration 5800 : Loss 3581.4690\n",
      "Iteration 5810 : Loss 3580.5572\n",
      "Iteration 5820 : Loss 3579.6472\n",
      "Iteration 5830 : Loss 3578.7390\n",
      "Iteration 5840 : Loss 3577.8326\n",
      "Iteration 5850 : Loss 3576.9280\n",
      "Iteration 5860 : Loss 3576.0252\n",
      "Iteration 5870 : Loss 3575.1242\n",
      "Iteration 5880 : Loss 3574.2249\n",
      "Iteration 5890 : Loss 3573.3274\n",
      "Iteration 5900 : Loss 3572.4317\n",
      "Iteration 5910 : Loss 3571.5378\n",
      "Iteration 5920 : Loss 3570.6456\n",
      "Iteration 5930 : Loss 3569.7552\n",
      "Iteration 5940 : Loss 3568.8665\n",
      "Iteration 5950 : Loss 3567.9796\n",
      "Iteration 5960 : Loss 3567.0945\n",
      "Iteration 5970 : Loss 3566.2110\n",
      "Iteration 5980 : Loss 3565.3293\n",
      "Iteration 5990 : Loss 3564.4493\n",
      "Iteration 6000 : Loss 3563.5711\n",
      "Iteration 6010 : Loss 3562.6945\n",
      "Iteration 6020 : Loss 3561.8197\n",
      "Iteration 6030 : Loss 3560.9466\n",
      "Iteration 6040 : Loss 3560.0752\n",
      "Iteration 6050 : Loss 3559.2055\n",
      "Iteration 6060 : Loss 3558.3375\n",
      "Iteration 6070 : Loss 3557.4712\n",
      "Iteration 6080 : Loss 3556.6065\n",
      "Iteration 6090 : Loss 3555.7436\n",
      "Iteration 6100 : Loss 3554.8823\n",
      "Iteration 6110 : Loss 3554.0227\n",
      "Iteration 6120 : Loss 3553.1647\n",
      "Iteration 6130 : Loss 3552.3085\n",
      "Iteration 6140 : Loss 3551.4538\n",
      "Iteration 6150 : Loss 3550.6009\n",
      "Iteration 6160 : Loss 3549.7495\n",
      "Iteration 6170 : Loss 3548.8999\n",
      "Iteration 6180 : Loss 3548.0518\n",
      "Iteration 6190 : Loss 3547.2054\n",
      "Iteration 6200 : Loss 3546.3607\n",
      "Iteration 6210 : Loss 3545.5175\n",
      "Iteration 6220 : Loss 3544.6760\n",
      "Iteration 6230 : Loss 3543.8361\n",
      "Iteration 6240 : Loss 3542.9978\n",
      "Iteration 6250 : Loss 3542.1611\n",
      "Iteration 6260 : Loss 3541.3261\n",
      "Iteration 6270 : Loss 3540.4926\n",
      "Iteration 6280 : Loss 3539.6607\n",
      "Iteration 6290 : Loss 3538.8304\n",
      "Iteration 6300 : Loss 3538.0017\n",
      "Iteration 6310 : Loss 3537.1746\n",
      "Iteration 6320 : Loss 3536.3491\n",
      "Iteration 6330 : Loss 3535.5251\n",
      "Iteration 6340 : Loss 3534.7027\n",
      "Iteration 6350 : Loss 3533.8819\n",
      "Iteration 6360 : Loss 3533.0626\n",
      "Iteration 6370 : Loss 3532.2449\n",
      "Iteration 6380 : Loss 3531.4288\n",
      "Iteration 6390 : Loss 3530.6141\n",
      "Iteration 6400 : Loss 3529.8011\n",
      "Iteration 6410 : Loss 3528.9896\n",
      "Iteration 6420 : Loss 3528.1796\n",
      "Iteration 6430 : Loss 3527.3711\n",
      "Iteration 6440 : Loss 3526.5642\n",
      "Iteration 6450 : Loss 3525.7588\n",
      "Iteration 6460 : Loss 3524.9549\n",
      "Iteration 6470 : Loss 3524.1525\n",
      "Iteration 6480 : Loss 3523.3517\n",
      "Iteration 6490 : Loss 3522.5523\n",
      "Iteration 6500 : Loss 3521.7545\n",
      "Iteration 6510 : Loss 3520.9582\n",
      "Iteration 6520 : Loss 3520.1633\n",
      "Iteration 6530 : Loss 3519.3700\n",
      "Iteration 6540 : Loss 3518.5781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6550 : Loss 3517.7877\n",
      "Iteration 6560 : Loss 3516.9988\n",
      "Iteration 6570 : Loss 3516.2114\n",
      "Iteration 6580 : Loss 3515.4254\n",
      "Iteration 6590 : Loss 3514.6409\n",
      "Iteration 6600 : Loss 3513.8579\n",
      "Iteration 6610 : Loss 3513.0763\n",
      "Iteration 6620 : Loss 3512.2962\n",
      "Iteration 6630 : Loss 3511.5176\n",
      "Iteration 6640 : Loss 3510.7403\n",
      "Iteration 6650 : Loss 3509.9646\n",
      "Iteration 6660 : Loss 3509.1903\n",
      "Iteration 6670 : Loss 3508.4174\n",
      "Iteration 6680 : Loss 3507.6459\n",
      "Iteration 6690 : Loss 3506.8759\n",
      "Iteration 6700 : Loss 3506.1073\n",
      "Iteration 6710 : Loss 3505.3401\n",
      "Iteration 6720 : Loss 3504.5744\n",
      "Iteration 6730 : Loss 3503.8100\n",
      "Iteration 6740 : Loss 3503.0471\n",
      "Iteration 6750 : Loss 3502.2855\n",
      "Iteration 6760 : Loss 3501.5254\n",
      "Iteration 6770 : Loss 3500.7667\n",
      "Iteration 6780 : Loss 3500.0094\n",
      "Iteration 6790 : Loss 3499.2534\n",
      "Iteration 6800 : Loss 3498.4989\n",
      "Iteration 6810 : Loss 3497.7457\n",
      "Iteration 6820 : Loss 3496.9939\n",
      "Iteration 6830 : Loss 3496.2435\n",
      "Iteration 6840 : Loss 3495.4945\n",
      "Iteration 6850 : Loss 3494.7468\n",
      "Iteration 6860 : Loss 3494.0005\n",
      "Iteration 6870 : Loss 3493.2556\n",
      "Iteration 6880 : Loss 3492.5120\n",
      "Iteration 6890 : Loss 3491.7698\n",
      "Iteration 6900 : Loss 3491.0289\n",
      "Iteration 6910 : Loss 3490.2894\n",
      "Iteration 6920 : Loss 3489.5512\n",
      "Iteration 6930 : Loss 3488.8144\n",
      "Iteration 6940 : Loss 3488.0789\n",
      "Iteration 6950 : Loss 3487.3447\n",
      "Iteration 6960 : Loss 3486.6119\n",
      "Iteration 6970 : Loss 3485.8804\n",
      "Iteration 6980 : Loss 3485.1502\n",
      "Iteration 6990 : Loss 3484.4213\n",
      "Iteration 7000 : Loss 3483.6938\n",
      "Iteration 7010 : Loss 3482.9676\n",
      "Iteration 7020 : Loss 3482.2426\n",
      "Iteration 7030 : Loss 3481.5190\n",
      "Iteration 7040 : Loss 3480.7967\n",
      "Iteration 7050 : Loss 3480.0757\n",
      "Iteration 7060 : Loss 3479.3560\n",
      "Iteration 7070 : Loss 3478.6376\n",
      "Iteration 7080 : Loss 3477.9204\n",
      "Iteration 7090 : Loss 3477.2046\n",
      "Iteration 7100 : Loss 3476.4900\n",
      "Iteration 7110 : Loss 3475.7768\n",
      "Iteration 7120 : Loss 3475.0648\n",
      "Iteration 7130 : Loss 3474.3540\n",
      "Iteration 7140 : Loss 3473.6446\n",
      "Iteration 7150 : Loss 3472.9364\n",
      "Iteration 7160 : Loss 3472.2295\n",
      "Iteration 7170 : Loss 3471.5238\n",
      "Iteration 7180 : Loss 3470.8194\n",
      "Iteration 7190 : Loss 3470.1162\n",
      "Iteration 7200 : Loss 3469.4143\n",
      "Iteration 7210 : Loss 3468.7136\n",
      "Iteration 7220 : Loss 3468.0142\n",
      "Iteration 7230 : Loss 3467.3161\n",
      "Iteration 7240 : Loss 3466.6191\n",
      "Iteration 7250 : Loss 3465.9234\n",
      "Iteration 7260 : Loss 3465.2289\n",
      "Iteration 7270 : Loss 3464.5357\n",
      "Iteration 7280 : Loss 3463.8437\n",
      "Iteration 7290 : Loss 3463.1529\n",
      "Iteration 7300 : Loss 3462.4633\n",
      "Iteration 7310 : Loss 3461.7749\n",
      "Iteration 7320 : Loss 3461.0878\n",
      "Iteration 7330 : Loss 3460.4018\n",
      "Iteration 7340 : Loss 3459.7171\n",
      "Iteration 7350 : Loss 3459.0336\n",
      "Iteration 7360 : Loss 3458.3512\n",
      "Iteration 7370 : Loss 3457.6701\n",
      "Iteration 7380 : Loss 3456.9902\n",
      "Iteration 7390 : Loss 3456.3114\n",
      "Iteration 7400 : Loss 3455.6339\n",
      "Iteration 7410 : Loss 3454.9575\n",
      "Iteration 7420 : Loss 3454.2823\n",
      "Iteration 7430 : Loss 3453.6083\n",
      "Iteration 7440 : Loss 3452.9354\n",
      "Iteration 7450 : Loss 3452.2638\n",
      "Iteration 7460 : Loss 3451.5933\n",
      "Iteration 7470 : Loss 3450.9239\n",
      "Iteration 7480 : Loss 3450.2558\n",
      "Iteration 7490 : Loss 3449.5888\n",
      "Iteration 7500 : Loss 3448.9229\n",
      "Iteration 7510 : Loss 3448.2582\n",
      "Iteration 7520 : Loss 3447.5947\n",
      "Iteration 7530 : Loss 3446.9323\n",
      "Iteration 7540 : Loss 3446.2711\n",
      "Iteration 7550 : Loss 3445.6110\n",
      "Iteration 7560 : Loss 3444.9520\n",
      "Iteration 7570 : Loss 3444.2942\n",
      "Iteration 7580 : Loss 3443.6375\n",
      "Iteration 7590 : Loss 3442.9819\n",
      "Iteration 7600 : Loss 3442.3275\n",
      "Iteration 7610 : Loss 3441.6742\n",
      "Iteration 7620 : Loss 3441.0220\n",
      "Iteration 7630 : Loss 3440.3710\n",
      "Iteration 7640 : Loss 3439.7211\n",
      "Iteration 7650 : Loss 3439.0722\n",
      "Iteration 7660 : Loss 3438.4245\n",
      "Iteration 7670 : Loss 3437.7779\n",
      "Iteration 7680 : Loss 3437.1324\n",
      "Iteration 7690 : Loss 3436.4881\n",
      "Iteration 7700 : Loss 3435.8448\n",
      "Iteration 7710 : Loss 3435.2026\n",
      "Iteration 7720 : Loss 3434.5615\n",
      "Iteration 7730 : Loss 3433.9215\n",
      "Iteration 7740 : Loss 3433.2826\n",
      "Iteration 7750 : Loss 3432.6448\n",
      "Iteration 7760 : Loss 3432.0080\n",
      "Iteration 7770 : Loss 3431.3724\n",
      "Iteration 7780 : Loss 3430.7378\n",
      "Iteration 7790 : Loss 3430.1043\n",
      "Iteration 7800 : Loss 3429.4719\n",
      "Iteration 7810 : Loss 3428.8405\n",
      "Iteration 7820 : Loss 3428.2102\n",
      "Iteration 7830 : Loss 3427.5810\n",
      "Iteration 7840 : Loss 3426.9529\n",
      "Iteration 7850 : Loss 3426.3258\n",
      "Iteration 7860 : Loss 3425.6997\n",
      "Iteration 7870 : Loss 3425.0748\n",
      "Iteration 7880 : Loss 3424.4508\n",
      "Iteration 7890 : Loss 3423.8280\n",
      "Iteration 7900 : Loss 3423.2061\n",
      "Iteration 7910 : Loss 3422.5854\n",
      "Iteration 7920 : Loss 3421.9656\n",
      "Iteration 7930 : Loss 3421.3469\n",
      "Iteration 7940 : Loss 3420.7293\n",
      "Iteration 7950 : Loss 3420.1126\n",
      "Iteration 7960 : Loss 3419.4970\n",
      "Iteration 7970 : Loss 3418.8825\n",
      "Iteration 7980 : Loss 3418.2689\n",
      "Iteration 7990 : Loss 3417.6564\n",
      "Iteration 8000 : Loss 3417.0450\n",
      "Iteration 8010 : Loss 3416.4345\n",
      "Iteration 8020 : Loss 3415.8250\n",
      "Iteration 8030 : Loss 3415.2166\n",
      "Iteration 8040 : Loss 3414.6092\n",
      "Iteration 8050 : Loss 3414.0028\n",
      "Iteration 8060 : Loss 3413.3974\n",
      "Iteration 8070 : Loss 3412.7930\n",
      "Iteration 8080 : Loss 3412.1896\n",
      "Iteration 8090 : Loss 3411.5872\n",
      "Iteration 8100 : Loss 3410.9858\n",
      "Iteration 8110 : Loss 3410.3854\n",
      "Iteration 8120 : Loss 3409.7860\n",
      "Iteration 8130 : Loss 3409.1875\n",
      "Iteration 8140 : Loss 3408.5901\n",
      "Iteration 8150 : Loss 3407.9937\n",
      "Iteration 8160 : Loss 3407.3982\n",
      "Iteration 8170 : Loss 3406.8037\n",
      "Iteration 8180 : Loss 3406.2102\n",
      "Iteration 8190 : Loss 3405.6177\n",
      "Iteration 8200 : Loss 3405.0261\n",
      "Iteration 8210 : Loss 3404.4355\n",
      "Iteration 8220 : Loss 3403.8459\n",
      "Iteration 8230 : Loss 3403.2573\n",
      "Iteration 8240 : Loss 3402.6696\n",
      "Iteration 8250 : Loss 3402.0829\n",
      "Iteration 8260 : Loss 3401.4971\n",
      "Iteration 8270 : Loss 3400.9123\n",
      "Iteration 8280 : Loss 3400.3285\n",
      "Iteration 8290 : Loss 3399.7456\n",
      "Iteration 8300 : Loss 3399.1636\n",
      "Iteration 8310 : Loss 3398.5826\n",
      "Iteration 8320 : Loss 3398.0026\n",
      "Iteration 8330 : Loss 3397.4235\n",
      "Iteration 8340 : Loss 3396.8453\n",
      "Iteration 8350 : Loss 3396.2681\n",
      "Iteration 8360 : Loss 3395.6918\n",
      "Iteration 8370 : Loss 3395.1165\n",
      "Iteration 8380 : Loss 3394.5420\n",
      "Iteration 8390 : Loss 3393.9686\n",
      "Iteration 8400 : Loss 3393.3960\n",
      "Iteration 8410 : Loss 3392.8244\n",
      "Iteration 8420 : Loss 3392.2536\n",
      "Iteration 8430 : Loss 3391.6839\n",
      "Iteration 8440 : Loss 3391.1150\n",
      "Iteration 8450 : Loss 3390.5470\n",
      "Iteration 8460 : Loss 3389.9800\n",
      "Iteration 8470 : Loss 3389.4139\n",
      "Iteration 8480 : Loss 3388.8487\n",
      "Iteration 8490 : Loss 3388.2843\n",
      "Iteration 8500 : Loss 3387.7209\n",
      "Iteration 8510 : Loss 3387.1585\n",
      "Iteration 8520 : Loss 3386.5969\n",
      "Iteration 8530 : Loss 3386.0362\n",
      "Iteration 8540 : Loss 3385.4764\n",
      "Iteration 8550 : Loss 3384.9175\n",
      "Iteration 8560 : Loss 3384.3595\n",
      "Iteration 8570 : Loss 3383.8023\n",
      "Iteration 8580 : Loss 3383.2461\n",
      "Iteration 8590 : Loss 3382.6908\n",
      "Iteration 8600 : Loss 3382.1363\n",
      "Iteration 8610 : Loss 3381.5828\n",
      "Iteration 8620 : Loss 3381.0301\n",
      "Iteration 8630 : Loss 3380.4783\n",
      "Iteration 8640 : Loss 3379.9273\n",
      "Iteration 8650 : Loss 3379.3773\n",
      "Iteration 8660 : Loss 3378.8281\n",
      "Iteration 8670 : Loss 3378.2798\n",
      "Iteration 8680 : Loss 3377.7323\n",
      "Iteration 8690 : Loss 3377.1857\n",
      "Iteration 8700 : Loss 3376.6400\n",
      "Iteration 8710 : Loss 3376.0952\n",
      "Iteration 8720 : Loss 3375.5512\n",
      "Iteration 8730 : Loss 3375.0081\n",
      "Iteration 8740 : Loss 3374.4658\n",
      "Iteration 8750 : Loss 3373.9244\n",
      "Iteration 8760 : Loss 3373.3838\n",
      "Iteration 8770 : Loss 3372.8441\n",
      "Iteration 8780 : Loss 3372.3052\n",
      "Iteration 8790 : Loss 3371.7672\n",
      "Iteration 8800 : Loss 3371.2300\n",
      "Iteration 8810 : Loss 3370.6937\n",
      "Iteration 8820 : Loss 3370.1582\n",
      "Iteration 8830 : Loss 3369.6235\n",
      "Iteration 8840 : Loss 3369.0897\n",
      "Iteration 8850 : Loss 3368.5567\n",
      "Iteration 8860 : Loss 3368.0246\n",
      "Iteration 8870 : Loss 3367.4932\n",
      "Iteration 8880 : Loss 3366.9628\n",
      "Iteration 8890 : Loss 3366.4331\n",
      "Iteration 8900 : Loss 3365.9043\n",
      "Iteration 8910 : Loss 3365.3762\n",
      "Iteration 8920 : Loss 3364.8491\n",
      "Iteration 8930 : Loss 3364.3227\n",
      "Iteration 8940 : Loss 3363.7971\n",
      "Iteration 8950 : Loss 3363.2724\n",
      "Iteration 8960 : Loss 3362.7485\n",
      "Iteration 8970 : Loss 3362.2254\n",
      "Iteration 8980 : Loss 3361.7031\n",
      "Iteration 8990 : Loss 3361.1816\n",
      "Iteration 9000 : Loss 3360.6609\n",
      "Iteration 9010 : Loss 3360.1410\n",
      "Iteration 9020 : Loss 3359.6219\n",
      "Iteration 9030 : Loss 3359.1036\n",
      "Iteration 9040 : Loss 3358.5862\n",
      "Iteration 9050 : Loss 3358.0695\n",
      "Iteration 9060 : Loss 3357.5536\n",
      "Iteration 9070 : Loss 3357.0385\n",
      "Iteration 9080 : Loss 3356.5242\n",
      "Iteration 9090 : Loss 3356.0107\n",
      "Iteration 9100 : Loss 3355.4980\n",
      "Iteration 9110 : Loss 3354.9861\n",
      "Iteration 9120 : Loss 3354.4749\n",
      "Iteration 9130 : Loss 3353.9645\n",
      "Iteration 9140 : Loss 3353.4550\n",
      "Iteration 9150 : Loss 3352.9462\n",
      "Iteration 9160 : Loss 3352.4381\n",
      "Iteration 9170 : Loss 3351.9309\n",
      "Iteration 9180 : Loss 3351.4244\n",
      "Iteration 9190 : Loss 3350.9187\n",
      "Iteration 9200 : Loss 3350.4138\n",
      "Iteration 9210 : Loss 3349.9096\n",
      "Iteration 9220 : Loss 3349.4062\n",
      "Iteration 9230 : Loss 3348.9036\n",
      "Iteration 9240 : Loss 3348.4017\n",
      "Iteration 9250 : Loss 3347.9006\n",
      "Iteration 9260 : Loss 3347.4003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9270 : Loss 3346.9007\n",
      "Iteration 9280 : Loss 3346.4019\n",
      "Iteration 9290 : Loss 3345.9038\n",
      "Iteration 9300 : Loss 3345.4065\n",
      "Iteration 9310 : Loss 3344.9099\n",
      "Iteration 9320 : Loss 3344.4141\n",
      "Iteration 9330 : Loss 3343.9190\n",
      "Iteration 9340 : Loss 3343.4247\n",
      "Iteration 9350 : Loss 3342.9312\n",
      "Iteration 9360 : Loss 3342.4383\n",
      "Iteration 9370 : Loss 3341.9462\n",
      "Iteration 9380 : Loss 3341.4549\n",
      "Iteration 9390 : Loss 3340.9643\n",
      "Iteration 9400 : Loss 3340.4744\n",
      "Iteration 9410 : Loss 3339.9853\n",
      "Iteration 9420 : Loss 3339.4969\n",
      "Iteration 9430 : Loss 3339.0092\n",
      "Iteration 9440 : Loss 3338.5223\n",
      "Iteration 9450 : Loss 3338.0361\n",
      "Iteration 9460 : Loss 3337.5506\n",
      "Iteration 9470 : Loss 3337.0658\n",
      "Iteration 9480 : Loss 3336.5818\n",
      "Iteration 9490 : Loss 3336.0985\n",
      "Iteration 9500 : Loss 3335.6159\n",
      "Iteration 9510 : Loss 3335.1340\n",
      "Iteration 9520 : Loss 3334.6529\n",
      "Iteration 9530 : Loss 3334.1724\n",
      "Iteration 9540 : Loss 3333.6927\n",
      "Iteration 9550 : Loss 3333.2137\n",
      "Iteration 9560 : Loss 3332.7354\n",
      "Iteration 9570 : Loss 3332.2578\n",
      "Iteration 9580 : Loss 3331.7810\n",
      "Iteration 9590 : Loss 3331.3048\n",
      "Iteration 9600 : Loss 3330.8293\n",
      "Iteration 9610 : Loss 3330.3546\n",
      "Iteration 9620 : Loss 3329.8805\n",
      "Iteration 9630 : Loss 3329.4071\n",
      "Iteration 9640 : Loss 3328.9345\n",
      "Iteration 9650 : Loss 3328.4625\n",
      "Iteration 9660 : Loss 3327.9913\n",
      "Iteration 9670 : Loss 3327.5207\n",
      "Iteration 9680 : Loss 3327.0508\n",
      "Iteration 9690 : Loss 3326.5816\n",
      "Iteration 9700 : Loss 3326.1131\n",
      "Iteration 9710 : Loss 3325.6453\n",
      "Iteration 9720 : Loss 3325.1782\n",
      "Iteration 9730 : Loss 3324.7118\n",
      "Iteration 9740 : Loss 3324.2460\n",
      "Iteration 9750 : Loss 3323.7810\n",
      "Iteration 9760 : Loss 3323.3166\n",
      "Iteration 9770 : Loss 3322.8529\n",
      "Iteration 9780 : Loss 3322.3898\n",
      "Iteration 9790 : Loss 3321.9275\n",
      "Iteration 9800 : Loss 3321.4658\n",
      "Iteration 9810 : Loss 3321.0048\n",
      "Iteration 9820 : Loss 3320.5445\n",
      "Iteration 9830 : Loss 3320.0848\n",
      "Iteration 9840 : Loss 3319.6258\n",
      "Iteration 9850 : Loss 3319.1675\n",
      "Iteration 9860 : Loss 3318.7099\n",
      "Iteration 9870 : Loss 3318.2529\n",
      "Iteration 9880 : Loss 3317.7965\n",
      "Iteration 9890 : Loss 3317.3409\n",
      "Iteration 9900 : Loss 3316.8859\n",
      "Iteration 9910 : Loss 3316.4315\n",
      "Iteration 9920 : Loss 3315.9778\n",
      "Iteration 9930 : Loss 3315.5248\n",
      "Iteration 9940 : Loss 3315.0724\n",
      "Iteration 9950 : Loss 3314.6207\n",
      "Iteration 9960 : Loss 3314.1697\n",
      "Iteration 9970 : Loss 3313.7192\n",
      "Iteration 9980 : Loss 3313.2695\n",
      "Iteration 9990 : Loss 3312.8204\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "for i in range(1, 10000):\n",
    "    dW, db = gradient(X_train, W, b, y_train)\n",
    "    W -= LEARNING_RATE * dW\n",
    "    b -= LEARNING_RATE * db\n",
    "    L = loss(X_train, W, b, y_train)\n",
    "    losses.append(L)\n",
    "    if i % 10 == 0:\n",
    "        print('Iteration %d : Loss %0.4f' % (i, L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f9514649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d23124",
   "metadata": {},
   "source": [
    "**(10) test 데이터에 대한 성능 확인하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8783e680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3013.2293084118733"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test 데이터 예측하여  loss값 확인\n",
    "prediction = model(X_test, W, b)\n",
    "mse = loss(X_test, W, b, y_test)\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab4b000",
   "metadata": {},
   "source": [
    "**(11) 정답 데이터와 예측한 데이터 시각화하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "09129c78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prediction과 실제 정답인 y_test를 비교하여 정답률 그래프 확인\n",
    "\n",
    "\n",
    "plt.scatter(X_test[:, 0], y_test)\n",
    "plt.scatter(X_test[:, 0], prediction)\n",
    "plt.show()  # x축에는 total_bill을, y축에는 각각 tip값과 prediction 값을 표시\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addca41f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1826c8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
